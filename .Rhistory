cor(mtcars)
swirl()
library(swirl())
swirl()
x1c<-simbias()
apply(x1c, 1, mean)
fit1<-lm(Fertility~Agriculture, data=Swiss)
fit1<-lm(Fertility~Agriculture, data="Swiss")
fit1<-lm(Fertility~Agriculture, data="swiss")
fit1<-lm(Fertility~Agriculture, data=swiss)
fit3<-lm(Fertility~Agriculture+Examination+Education, data=swiss)
anova(fit1, fit3)
deviance(fit3)
d<-deviance(fit3)/43
n<-deviance(fit1)-deviance(fit3)
n <- (deviance(fit1) - deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail=FALSE)
shapiro.test(fit3$residuals)
anova(fit1, fit3, fit5, fit6)
View(ravenData)
mdl<-glm(ravenWinNum ~ ravenScore+family+ravenData)
mdl<-glm(ravenWinNum ~ ravenScore+family, data=ravenData)
mdl<-glm(ravenWinNum ~ ravenScore, family=binomial, data=ravenData)
lodds<-predict(mdl, data.frame(ravenScore=c(0, 3, 6))
)
exp(lodds)/(1+exp(lodds))
summary(mdl)
confint(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(0.95, 1)รง
qchisq(0.95, 1)
library(MASS)
library(MASS)head(shuttle)
library(MASS)
head(shuttle)
?shuttle
str(shuttle)
data(shuttle)
str(shuttle)
View(shuttle)
fit <- glm(use ~ wind, family='binomial', shuttle)
exp(fit$coeff)
source('~/.active-rstudio-document')
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
fit1 <- glm(use ~ wind - 1, data = shuttle, family = "binomial")
summary(fit)
windhead <- fit1$coef[1]
windtail <- fit1$coef[2]
exp(windtail)/exp(windhead)
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
summary(fit)
windhead2 <- fit2$coef[1]
windtail2 <- fit2$coef[2]
exp(windtail2)/exp(windhead2)
windtail2 <- fit2$coef[2]
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
summary(fit)
windhead2 <- fit2$coef[1]
fit2
fit1 <- glm(use ~ wind - 1, data = shuttle, family = "binomial")
shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
library(MASS)
data(shuttle)
# convert outcome to 0 = noauto, 1 = auto
#shuttle$use <- factor(shuttle$use, levels = c("auto", "noauto"), labels = c(1, 0))
fit1 <- glm(use ~ wind - 1, data = shuttle, family = "binomial")
summary(fit)
windhead <- fit1$coef[1]
windtail <- fit1$coef[2]
exp(windtail)/exp(windhead)
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
summary(fit)
windhead2 <- fit2$coef[1]
windtail2 <- fit2$coef[2]
exp(windtail2)/exp(windhead2)
shuttle$auto <- as.numeric(shuttle$use=="auto")
fit <- glm(auto ~ wind,  binomial,  shuttle)
fit3 <- glm(1-auto ~ wind,  binomial, shuttle)
fit$coefficients
fit3$coefficients
data(InsectSprays)
fit <- glm(count ~ spray  - 1, family = "poisson", data = InsectSprays)
exp(fit$coef[1])/exp(fit$coef[2])
View(InsectSprays)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
lhs <- function(x) ifelse(x < 0,0-x,0) # basis function 1 (lhs = left hockey stick)
rhs <- function(x) ifelse(x > 0,x-0,0) # basis function 2 (rhs = right hockey stick)
gb <- lm(y ~ lhs(x) + rhs(x))
x <- seq(-5,5,by=1)
py <- gb$coef[1]+gb$coef[2]*lhs(x)+gb$coef[3]*rhs(x)
lines(x, py)
library(swirl)
swirl()
var(rpois(1000, 50))
nxt()
head(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl <- glm(visits ~ date, poisson, hits)
summary(mdl)
onfint(mdl,'date')
confint(mdl,'date')
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,].
hits[704,]
lambda<-mdl$fitted.values[704]
qpois(.95, lambda)
mdl2 <- glm(visits ~ date, poisson, hits)
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits + 1))
qpois(.95, mdl2$fitted.values[704])
log(10)
library(UsingR)
data(mtcars)
mtcars$cyl <- factor(mtcars$cyl)
mtcars$vs <- factor(mtcars$vs)
mtcars$gear <- factor(mtcars$gear)
mtcars$carb <- factor(mtcars$carb)
mtcars$am <- factor(mtcars$am,labels=c("Automatic","Manual"))
fit1<-lm(mpg~am, data=mtcars)
fit2<-lm(mpg~., data=mtcars)
fit3<-step(lm(mpg~., data=mtcars), direction = "both")
fit4<- step(fit2, k=log(nrow(mtcars)))
summary(fit4)
summary(fit3)
anova(fit1,fit4,fit2)
+
with_interaction<-lm(mpg ~ wt + qsec + am + wt:am, data=mtcars)
with_interaction<-lm(mpg ~ wt + qsec + am + wt:am, data=mtcars)
fit5<-lm(mpg ~ wt + qsec + am + wt:am, data=mtcars) #with_interaction
anova(fit1,fit4,fit5)
anova(fit1,fit5,fit4)
2.936531 * 2
ggpairs(mtcars, main="Pairwise plot of mtcars dataset")
library(ggplot2)
library(GGally)
library(GGally)
install.packages(GGally)
ggpairs(mtcars, main="Pairwise plot of mtcars dataset")
install.packages("GGally")
library(GGally)
ggpairs(mtcars, main="Pairwise plot of mtcars dataset")
ggpairs(mtcars)
anova(fit4,fit5)
anova(fit5,fit4)
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
p=0.75, list=FALSE)
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type                     p=0.75, list=FALSE)
inTrain <- createDataPartition(y=spam$type,
p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
dim(testing)
inTrain <- createDataPartition(y=spam$type, p=0.60, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(testing)
inTrain <- createDataPartition(y=spam$type, p=0.60, list=true)
inTrain <- createDataPartition(y=spam$type, p=0.60, list=TRUE)
inTrain <- createDataPartition(y=spam$type, p=0.60, list=FALSE)
View(inTrain)
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
list=TRUE,returnTrain=TRUE)
sapply(folds,length)
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
list=FALSE,returnTrain=TRUE)
sapply(folds,length)
FOLDS
fold
folds
set.seed(32323)
folds <- createFolds(y=spam$type,k=10,
list=FALSE,returnTrain=FALSE)
sapply(folds,length)
folds
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL<-grep("^IL", names(training))
training2<-training[,c(1,IL)]
View(training2)
set.seed(3433)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
View(predictors)
adData = data.frame(diagnosis,predictors)
View(adData)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
inTrain2 = createDataPartition(adData$diagnosis, p = 3/4, list=FALSE)
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL<-grep("^IL", names(training))
training2<-training[,c(1,IL)]
View(training2)
prop.table(table(training))
prop.table(table(add))
prop.table(table(adData))
prop.table(table(adData$diagnosis))
prop.table(table(training$diagnosis))
nearZeroVar(training)
nearZeroVar(adData)
nearZeroVar
cor(training)
View(training)
cor(training[,-1])
View(training)
training[,-1]
ejem<-training[,-1]
ejem<-training2[,-1]
cor(ejem)
descor<-cor(ejem)
altas<-findCorrelation(descor, 0.90)
altas
altas<-findCorrelation(descor, 0.80)
altas
altas<-findCorrelation(descor, 0.50)
altas
preProc <- preProcess(training2, method="pca", thresh = 0.80)
preProc <- preProcess(training2[,-1], method="pca", thresh = 0.80)
prePoc
preProc
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL<-grep("^IL", names(training))
data<-training[,c(IL)]
preProc <- preProcess(data, method="pca", thresh = 0.80)
summary(preProc)
preProc
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(log10(training$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL<-grep("^IL", names(training))
training2<-training[,c(1,IL)]
modelFit <- train(training2$diagnosis ~ .,method="glm", data=training2)
confusionMatrix(testing$diagnosis,predict(modelFit,testing))
trainPC <- predict(preProc, training2$diagnosis)
preProc <- preProcess(training2[,-1], method="pca", thresh = 0.80)
trainPC <- predict(preProc, training2$diagnosis)
trainPC <- predict(preProc, training2[,-1])
preProc <- preProcess(training2[,-1], method="pca", thresh = 0.80)
trainPC <- predict(preProc, training2[,-1])
modelFit2 <- train(training2$diagnosis ~ .,method="glm", data=trainPC)
confusionMatrix(testing$diagnosis,predict(modelFit2,testing))
confusionMatrix(testing$diagnosis, predict(modelFit2,testing))
trainPC
trainPC <- predict(preProc, training2)
training2[,-1]
training2[,-1]+1
training2[,-1]
preProc <- preProcess(training2[,-1], method="pca", thresh = 0.80)
trainPC <- predict(preProc, training2[,-1])
modelFit2 <- train(trainPC$diagnosis ~ .,method="glm", data=trainPC)
modelFit2 <- train(trainPC$diagnosis ~ .,method="glm", data=trainPC)
modelFit2 <- train(training2$diagnosis ~ .,method="glm", data=trainPC)
modelFit2
modelFit
modelFit2
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL<-grep("^IL", names(training))
training2<-training[,c(1,IL)]
modelFit <- train(training2$diagnosis ~ .,method="glm", data=training2)
confusionMatrix(testing$diagnosis,predict(modelFit,testing))
preProc <- preProcess(training2[,-1], method="pca", thresh = 0.80)
trainPC <- predict(preProc, training2[,-1])
preProc <- preProcess(training2[,-1], method="pca", thresh = 0.80)
trainPC <- predict(preProc, training2[,-1])
modelFit2 <- train(training2$diagnosis ~ .,method="glm", data=trainPC)
confusionMatrix(testing$diagnosis, predict(modelFit2,testing))
IL<-grep("^IL", names(testing))
testing2<-testing[,c(1,IL)]
testPC <- predict(preProc, testing2[,-1])
confusionMatrix(testing$diagnosis, predict(modelFit2,testing))
confusionMatrix(testing$diagnosis, predict(modelFit2,testPC))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(log10(training$Superplasticizer))
hist((training$Superplasticizer))
mean(training$Superplasticizer)
hist(training$Superplasticizer-mean(training$Superplasticizer)/sd(training$Superplasticizer))
hist((training$Superplasticizer-mean(training$Superplasticizer)/sd(training$Superplasticizer))
)
hist(scale(training$Superplasticizer))
hist((training$Superplasticizer-mean(training$Superplasticizer)/sd(training$Superplasticizer))
)hist(scale(training$Superplasticizer))
hist(scale(training$Superplasticizer))
hist(log10(training$Superplasticizer))
qqplot(hist(scale(training$Superplasticizer))
qqplot(hist(scale(training$Superplasticizer)))
qqplot(hist(scale(training$Superplasticizer)))
qqnorm(hist(scale(training$Superplasticizer)))
qqnorm(training$Superplasticizer)
qqnorm(log10(training$Superplasticizer))
qqnorm(log10(training$Superplasticizer))
qqnorm(training$Superplasticizer)
hist(log10(training$Superplasticizer))
hist(scale(training$Superplasticizer))
skewness(training$Superplasticizer)
library(moments)
library(moments)
install.packages("moments")
library(moments)
skewness(training$Superplasticizer)
kurtosis(training$Superplasticizer)
log(0)
hist((training$Superplasticizer))
hist(log10(training$Superplasticizer))
library(AppliedPredictiveModeling)
library(caret)
library(Hmisc)
library(ggplot2)
data(concrete)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
nrow(training)
qplot(c(1:774), CompressiveStrength, color=cut2(FlyAsh, g=4), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(Age, g=4), data=training)
View(training)
qplot(c(1:774), CompressiveStrength, color=cut2(Cement, g=4), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(BlastFurnaceSlag, g=4), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(Water, g=4), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(Superplasticizer, g=4), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(CoarseAggregate, g=4), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(FineAggregate, g=4), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(CompressiveStrength, g=4), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(CompressiveStrength, g=3), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(CompressiveStrength, g=5), data=training)
qplot(c(1:774), CompressiveStrength, color=cut2(CompressiveStrength, g=4), data=training)
nrow(training)
qplot(c(1:nrow(training)), CompressiveStrength, color=cut2(CompressiveStrength, g=4), data=training)
quantile(training$CompressiveStrength)
qplot(CompressiveStrength, c(1:nrow(training)), color=cut2(CompressiveStrength, g=4), data=training)
qplot(CompressiveStrength, c(1:nrow(training)), color=cut2(Age, g=4), data=training)
qplot(CompressiveStrength, c(1:nrow(training)), color=cut2(FlyAsh, g=4), data=training)
qplot(CompressiveStrength, c(1:nrow(training)), color=cut2(CompressiveStrength, g=4), data=training)
row(training)
qplot(CompressiveStrength, row(training), color=cut2(CompressiveStrength, g=4), data=training)
FlyAsh
qplot(CompressiveStrength, c(1:nrow(training)), color=cut2(FlyAsh, g=4), data=training)
qplot(CompressiveStrength, c(1:nrow(training)), color=cut2(FlyAsh, g=4), data=training)
splitOn <- cut2(training$Age, g = 4)
splitOn <- mapvalues(splitOn,
from = levels(factor(splitOn)),
to = c("red", "blue", "yellow", "green"))
# automatically includes index of samples
plot(training$CompressiveStrength, col = splitOn)
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
Superplasticizer
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
skewness(training$Superplasticizer)
hist((training$Superplasticizer))
View(training)
hist((testing$Superplasticizer))
hist((training$Superplasticizer))
hist((testing$Superplasticizer))
hist(log10(training$Superplasticizer))
hist(log10(training$Superplasticizer)+1)
hist(log10(training$Superplasticizer)+1)
log(0)
log10(0)
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
ejem<-log10(training$Superplasticizer)
ejem
plot(ejem)
hist(ejem)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL<-grep("^IL", names(training))
data<-training[,c(IL)]
preProc <- preProcess(data, method="pca", thresh = 0.90)
preProc
library(caret)
setwd("~/machine")
train<-read.csv("pml-training.csv", na.strings = c("NA",""," "))
setwd("~/coursera/machine")
train<-read.csv("pml-training.csv", na.strings = c("NA",""," "))
train<-read.csv("pml-training.csv", na.strings = c("NA",""," "))
test<-read.csv("pml-testing.csv", na.strings = c("NA",""," "))
#clean the dataset. Eliminate all NAs columns
nacol.train<-which(colSums(is.na(train))>0)
nacol.test<-which(colSums(is.na(test))>0)
train<-train[,-c(nacol.train)]
test<-test[,-c(nacol.test)]
#eliminate first 7 columns they doesn't bring any important information
train<-train[,-c(1:7)]
test<-test[,-c(1:7)]
#divide the training data in training and testings for crossvalidation
inTrain<-createDataPartition(train$classe, p = 0.7, list=FALSE)
training<-train[inTrain,]
testing<-train[-inTrain,]
#testing is the unused data to make the cross validation of the model fit
#fit the model
set.seed(12345)
#model <- randomForest(classe ~ ., data = training, ntree=500, importance=TRUE)
model<- train(classe ~ .,data=training, method="rf",ntree=500, trControl=trainControl(method="cv",number=5))
library(randomForest)
model<- train(classe ~ .,data=training, method="rf",ntree=500, trControl=trainControl(method="cv",number=5))
pred<- predict(model, testing)
#check the accuracy of the model fit
acc<-confusionMatrix(pred, testing$classe)
acc #accuracy high enough
library(rattle)
fancyRpartPlot(model$finalModel)
fancyRpartPlot(model$forest)
model
plot (model)
varImp(model)
model$modelInfo
model$finalModel
fancyRpartPlot(model$finalModel)
fancyRpartPlot(model$forest)
plot(model$finalModel, uniform=TRUE,
main="Classification Tree")
text(model$finalModel, use.n=TRUE, all=TRUE, cex=.8)
fancyRpartPlot(model$finalModel)
library(rattle)
fancyRpartPlot(model$finalModel)
plot(model, uniform=TRUE, main="Classification Tree")
plot(model$finalModel, uniform=TRUE, main="Classification Tree")
model ##Cross-Validated (5 fold)
text(model$finalModel, use.n=TRUE, all=TRUE, cex=.8)
fancyRpartPlot(model$finalModel)
getTree(model$finalModel,k=2)
accuracy(model,test)
model$method
model$modelInfo
model$modelType
model$results
model$pred
model$bestTune
model$call
model$dots
model$control
model$finalModel
model$xlevels
plot(model$finalModel, uniform=TRUE, main="Classification Tree")
fancyRpartPlot(model)
fancyRpartPlot(model$finalModel)
getTree(model$finalModel,k=2)
fancyRpartPlot(model$finalModel)
plot(getTree(model$finalModel,k=2))
varImpPlot(model)
varImpPlot(model$finalModel)
varImpPlot(model$finalModel, color="red")
varImp(model$finalModel)
plot(varImp(model), top=20)
fancyRpartPlot(model)
fancyRpartPlot(model$finalModel)
o
trainUrl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv""
trainUrl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
trainUrl<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train<-read.csv(trainUrl, na.strings = c("NA",""," "))
train<-read.csv("pml-training.csv", na.strings = c("NA",""," "))
dim(test)
